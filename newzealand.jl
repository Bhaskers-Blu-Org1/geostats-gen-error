import Pkg; Pkg.activate(".")

using GeoStats
using DataFrames
using CSV
using MLJ
using Distributed
using DensityRatioEstimation
# using ProgressMeter
using Random
using LossFunctions

# read raw data
df = CSV.read("data/new_zealand/logs_no_duplicates.csv")
df.FORMATION = categorical(df.FORMATION)
df.ONSHORE   = categorical(df.ONSHORE)

# data for formation classification
dfc = dropmissing(df[[:X,:Y,:Z,:GR,:SP,:DENS,:DTC,:TEMP,:FORMATION,:ONSHORE]])

# create spatial data
wells = GeoDataFrame(dfc, [:X,:Y,:Z])

variables(wells)

npoints(wells)

formations = groupby(wells, :FORMATION)

fvalues = get.(formations[:values])
fsizes  = length.(subsets(formations))

ordforms = sortperm(fsizes, rev=true)

G1 = ordforms[1:2]
G2 = ordforms[3:4]
G3 = ordforms[5:end];


Œ© = DataCollection(formations[G1])

groups = groupby(Œ©, :ONSHORE)

# onshore (True) first and offshore (False) last
ordered = sortperm(groups[:values], rev=true)

Œ©s, Œ©t = groups[ordered]

allvars = keys(variables(wells))
discard = [:WELL_NAME,:DIRECTIONAL_SURVEY,:ONSHORE,:DEPT,:BS, :FORMATION]
numeric = collect(setdiff(allvars, discard))


# t = RegressionTask((:GR,:DENS,:DTC,:TEMP,:RESD), :SP)
subtypes(AbstractErrorEstimator)


function error_cv(m, p, k, loss)
    s = PointwiseLearn(m)
    v = CrossValidation(k, loss=loss)
    error(s, p, v)
end

function error_bv(m, p, r·µ¶, loss)
    s = PointwiseLearn(m)
    v = BlockCrossValidation(r·µ¶, loss=loss)
    error(s, p, v)
end

function error_wv(m, p, k, loss, œÉ=15.,b=10)
    s = PointwiseLearn(m)
    v = DensityRatioValidation(k, estimator=LSIF(œÉ=œÉ,b=b), loss=loss)
    error(s, p, v)
end

function error_empirical(m, p, Œ©ts, loss)
    ≈∑ = solve(p, PointwiseLearn(m))
    y = targetdata(p)
    result = Dict()
    for (col, ùîè) in loss
        result[col] = LossFunctions.value(ùîè, y[col], ≈∑[col], AggMode.Mean())
    end
    result
end

function error_comparison(m, p, Œ©ts, r·µ¶, k, loss, col)
    # parameters for validation methods
    #@assert r·µ¶ ‚â• r "block size smaller than correlation length"

    # try different error estimates
    loss_dict = Dict(col => loss)
    println("Performing CV")
    @time cv = error_cv(m, p, k, loss_dict)[col]
    @show cv
    println("Performing BCV")
    @time bv = error_bv(m, p, r·µ¶, loss_dict)[col]
    drv_results = Dict()
    for œÉ in [1., 5., 10., 15.,  20., 25.]
        println("Performing DRV with œÉ=$(œÉ)")
        try
            @time drv = error_wv(m, p, k, loss_dict, œÉ)[col]
            drv_results[Symbol("DRV_$(Int(œÉ))")] = drv
        catch e
            println("skipping DRV - invalid œÉ")
            println(e)
        end
    end

    # true error
    actual = error_empirical(m, p, Œ©ts,loss_dict)[col]

    merge((r·µ¶=r·µ¶, k=k, CV=cv, BCV=bv),
          drv_results,
          (ACTUAL=actual, MODEL=info(m).name, target=col))
end

#TODO find the best r·µ¶ using variograms
#EmpiricalVariogram(Œ©s, :TEMP)
r·µ¶=500
k = length(GeoStats.partition(Œ©s, BlockPartitioner(r·µ¶)))
show_all(x) = show(stdout, "text/plain", x)

# --------
# all_class_models = models(m->m.is_pure_julia && m.is_supervised &&
#                           m.target_scitype == AbstractVector{<:Finite})
# show_all(all_class_models)
#
# @load DecisionTreeClassifier
# @load KNNClassifier
# # mrange = [DecisionTreeClassifier(), KNNClassifier()]
# class_models = [DecisionTreeClassifier(), KNNClassifier()]
# loss = ZeroOneLoss()
# t = ClassificationTask((:GR,:SP,:DENS,:DTC,:TEMP), :FORMATION)
# problem = LearningProblem(Œ©s, Œ©t, t)
#
# Random.seed!(42)
#
# class_results = DataFrame()
#
# for model in class_models#, Œ¥ in Œ¥range, œÑ in œÑrange, r in rrange, œÅ in œÅrange
#     @show model, r·µ¶, k#, Œ¥, œÑ, r, œÅ
#     try
#         result = DataFrame([error_comparison(model, problem, Œ©t, r·µ¶, k, loss, :FORMATION)
#                             for i in 1:1])
#         append!(class_results, result)
#     catch e
#         println("skipped")
#         println(e)
#     end
# end
#
# CSV.write("results/new_zealand_classification.csv", class_results)
# println("Classification comparison is done!")
# --------------------
all_reg_models = models(m->m.is_pure_julia && m.is_supervised &&
                        m.target_scitype == AbstractArray{Continuous,1})

show_all(all_reg_models)
Random.seed!(42)

reg_results = DataFrame()

@load LinearRegressor pkg="MLJLinearModels"
@load DecisionTreeRegressor pkg="DecisionTree"
@load RandomForestRegressor pkg="DecisionTree"
@load KNNRegressor

reg_models = [LinearRegressor(), DecisionTreeRegressor(),
              RandomForestRegressor(), KNNRegressor()]
loss = L2DistLoss()

for model in reg_models, target in numeric

    t = RegressionTask(numeric[numeric .!= target], target)
    problem = LearningProblem(Œ©s, Œ©t, t)

    @show model, target, task#, Œ¥, œÑ, r, œÅ
    try
        result = DataFrame([error_comparison(model, problem, Œ©t, r·µ¶, k, loss, target)
                            for i in 1:1])
        append!(reg_results, result)
        @show result
    catch e
        println("skipped")
        println(e)
    end
    CSV.write("results/new_zealand_regression_drv.csv", reg_results)
end

# CSV.write("results/new_zealand_regression.csv", reg_results)
